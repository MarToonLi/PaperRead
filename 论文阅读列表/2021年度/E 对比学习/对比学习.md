对比学习

## 《A Simple Framework for Contrastive Learning of Visual Representations》

### 摘要

提出了一个不需要特殊结构或者内存库的对比自监督学习算法。对其学习到有用表示信息的方式是，系统地分析SimCLR框架的主要组成部分。

- 数据增强方式在定义有效决策任务中，扮演了很重要的角色；
- 表征和对比损失之间的一个可学习的非线性转换，持续提升学到的表征的质量；
- 较大的batch_size和更多的参数迭代次数有利于对比学习的表征学习（相比于监督学习）；

### 引言

 无监督的学习到有效的视觉表征是一个长期存在的问题。主流的方法分为两类：生成式和判别式。

- 生成式方法在输入空间中生成模型的像素或者通过其他方式对像素进行建模。但是像素级别的生成方法是计算昂贵的，同时对于表征学习而言可能并不必要。
- 判别时方法借助与监督学习中使用的那些目标函数以学习表征，但是训练网络执行pretext任务，其中pretext任务中的标签和输入来源于为被标记的数据集。

许多方法通过启发式来设计pretext任务，但是这种设计方式限制了以学习到的表示的泛化性。

**而基于潜在空间中的对比学习的判别式方法已经显现出很大的潜力**。**而这里我们要引入的SimCLR框架要更加简单，同时效能达到当前前沿技术水平。**—— **简单**在于：既不需要特殊的架构，也不需要内存块。

本文的突出贡献点：

- 多中数据增强操作有利于产生有效的表征，同时无监督学习在该模型下的受益程度要远远高于监督学习。
- 学习到的表征和对比损失函数之间的 **一个可学习性的非线性转换** 提升了学习到的表征的质量；
- 通过对比交叉熵损失函数进行表征学习得益于归一化签入和适当调整的温度参数temperature parameter。
- 相比于监督学习，对比学习得益于更大的批量和更长的训练时间；同时，与监督学习一样，对比学习得益于更深和更宽的网络模型。

### 方法

#### 对比学习框架

SimCLR框架借助一个对比损失函数，通过最大化相同数据下不同增强方式的数据的一致性，以学习潜在空间中的表征。

<img src="/home/cold/PaperReadFastly/PaperRead/论文阅读列表/2021年度/E 对比学习/对比学习.assets/image-20210915205223279.png" alt="image-20210915205223279" style="zoom:80%;" />

**思路是：通过F和G函数得到的Z通过对比损失函数的训练得到一个不错的representationH，自监督学习结束后，通过F函数和其他下游函数的监督训练得到想要的结果。**

主要的工作：

- 随机的数据增强模块：**随机转换任意给定的数据样本以得到该相同样本的两种不同相关视图view。同时这两个视图被称之为一对正例。**

    本文中**依次sequentially**使用了三种增强方式：

    - 随机裁剪，紧跟resize至原始大小；
    - 随机色彩变形color distortions；
    - 随机高斯滤波

    实验表明，随机裁剪和颜色扭曲对性能的增值幅度很大。

- 基础的神经网络编码器F：从增强的数据样本中提取表征向量。

    本文使用了残差网络，同时获得的H表征是经过了平均池化层的。

- 小型的神经网络projection head映射头G：将学到的表征H映射到对比损失函数所 **能**应用到的空间。

    利用了两个FC作为隐藏层以获取Z。

    **实验表明，利用Z进行损失函数计算要优于使用H。**

- 一个对比损失函数：为对比预测任务所定义。

    **对比预测任务旨在，在包含了数据增强的样本集S中，对于给定的增强样本x，辨识出S中x对应的另一个x。**

相关符号声明：

- batch_size中含有N个样本。经过一对数据增强，得到2N个数据样本。**作者没有对负例样本明确地采样。相反地，对于一对正例，作者将一个batch中剩下的2(N-1)个增强样本视为负例。**

- 用余弦相似度作为一对正例的表征的相似度。同时，损失函数定义为：

    softmax+log+T温度参数+01指示函数。

    ![image-20210915212507806](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/2021年度/E 对比学习/对比学习.assets/image-20210915212507806.png)

    为方便起见，我们将其命名为NT Xent（**归一化温度缩放**交叉熵损失）。

    > 深度学习中的temperature parameter是什么 https://zhuanlan.zhihu.com/p/132785733
    >
    > `t`越大，结果越平滑`*t*`*的大小和模型最终模型的正确率没有直接关系，*我们可以将`t`的作用类比于学习率。我们的label类似于[1,0,0]，最“尖锐”，如果我们在训练时将*`*t*`*设置比较大，那么预测的概率分布会比较平滑，**那么loss会很大，这样可以避免我们陷入局部最优解（因为局部最优解一般都是loss比较低的位置。）**。随着训练的进行，我们将`t`变小，也可以称作降温，类似于**模拟退火**算法，这也是为什么要把`t`称作温度参数的原因。**变小模型才能收敛**。比如我们可以这这样设置`t`:
    >
    > ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau%3D%5Cfrac%7B%5Ctau_0%7D%7B1%2B%5Clog%7BT%7D%7D)
    >
    > 这里的`T`表示的是训练循环的次数。

