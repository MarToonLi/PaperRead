## 胶囊网络

### Dynamic Routing Between Capsules

这里谈及的向量版的加权求和过程，因为涉及C_ij的加权，因此是否这种操作与non-local类似呢？似乎也涉及了一个巨大的权重矩阵。

![img](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/2021年度/F 胶囊网络/胶囊网络.assets/v2-8cfd484b3283010c6cf1be594d1588b7_1440w.jpg)

#### 胶囊和普通神经元的结构差别：

卷积网络的特点：

- 输入输出数据；

    每个样本的特征由一系列标量表达；

- 中间操作：
    - 加权、求和；
    - 非线性激活；
- 输入输出的维度由权重矩阵决定；

胶囊网络的特点：

- 输入输出数据：

    每个样本的特征由一系列的向量表达；

- 中间操作：

    - 输入数据的放射变换；
    - 加权求和；
    - 非线性激活；

- 输入输出数据的维度变换由仿射变换矩阵决定；

注意：虽然中间操作由重合，但是具体操作公式均不相同。

<img src="/home/cold/PaperReadFastly/PaperRead/论文阅读列表/2021年度/F 胶囊网络/胶囊网络.assets/image-20210914100020421.png" alt="image-20210914100020421" style="zoom:80%;" />

#### 卷积网络的不足

- 一张脸的识别，CNN会依赖于眼睛、鼻子和嘴是否被呈现，作为一个indicator指示器，以考虑图片中是否含有一张脸；

    **但是并不会认识到这些组件的方向和空间相对位置**。

    需要了解的一件重要事情是，高级特征将低级特征组合为加权和：前一层的激活乘以后一层神经元的权重并相加，然后再传递给激活非线性。**在该设置中，组成更高级别特征的更简单特征之间不存在姿势（平移和旋转）关系**。

    **CNN解决此问题的方法是使用最大池或连续卷积层**，以减少流经网络的数据的空间大小，从而增加高层神经元的“视野”，从而允许它们在输入图像的更大区域中检测高阶特征。Max pooling是使卷积网络工作得出奇好的一个支柱，在许多领域实现了超人的性能。但不要被它的性能所愚弄：尽管CNN比之前的任何型号都好，但max pooling仍在丢失有价值的信息。

    **注意：上面说CNN解决组件关系这一特征学习的方式是：最大池化层和连续卷积操作，这也就意味着，胶囊网络解决该问题并不是通过增加胶囊网络层数或者是最大池化以获取组件之间的特征，而是通过少层即可实现。**

    **TODO：CNN的层级结构有着简单特征和高级特征之分，但是胶囊网络似乎这个语义解释方面，不是很了解。仅仅是最后一层的胶囊的语义解释比较清洗：对应每一个数字。**

- 卷积神经网络的内部数据表示不考虑**简单和复杂对象之间的重要空间层次**。

#### 图形学的背景内容

- 计算机图像学中的渲染

    计算机图形学处理的是从几何数据的一些内部层次表示中构造视觉图像。请注意，此表示的结构需要考虑对象的相对位置。这种**内部表示**作为几何对象的数组和表示这些对象的相对位置和方向的矩阵存储在计算机内存中。然后，**特殊的软件将该表示形式转换为屏幕上的图像。这称为渲染**。

- 受此启发，辛顿认为大脑实际上与渲染相反。他称之为逆向图形：通过眼睛接收到的**整体的视觉信息**，他们**解构**了我们周围世界的层次表示，并**试图将其与存储在大脑中已经学会的模式和关系相匹配**。这就是认知发生的方式。**关键的想法是大脑中物体的表现并不取决于视角**。

- 所以在这一点上，问题是：我们如何在神经网络中建立这些层次关系的模型？答案来自计算机图形学。在三维图形中，三维对象之间的关系可以用所谓的姿势来表示，**姿势本质上是平移加旋转**。

    Hinton认为，为了正确地进行分类和对象识别，保持对象部分之间的层次式姿势关系非常重要。这是一个关键的直觉，可以让你理解为什么胶囊理论如此重要。它包含对象之间的相对关系，并以**4D姿势矩阵**的形式进行数值表示

    

- 胶囊方法的另一个好处是，它**能够通过仅使用CNN将使用的数据的一小部分来学习达到最先进的性能**（Hinton在他关于CNN的错误的著名演讲中提到了这一点）。从这个意义上说，胶囊理论更接近于人脑在实践中的行为。为了学会区分数字，人脑只需要看到几十个例子，最多几百个。另一方面，CNN需要数以万计的例子来实现非常好的性能，这似乎是一种蛮力方法，显然不如我们用大脑所做的

- 胶囊网络的想法本身不是新鲜的事情，而没有出版的原因是因为没有技术能够实现：一方面，计算能力不足；另一方面，没有能够实现并成功学习胶囊网络的算法。而目前有了——胶囊动态路由算法，它允许胶囊之间相互通信。

    **CNN的卷积层也可以相同通信呀！**



#### 胶囊网络的结构

- 第一层：卷积层，28 X 28 X 1的输入，经过9X9，1，256的卷积层，得到20 X 20 X 256的输出；

    作用：充当局部特征检测器，将像素强度pixel intensities，转换为检测到的特征激活值，作为初级胶囊的输入。

- 第二层：初级胶囊层，

    作用：作为实体的多维表示的初级阶段，20 X 20 X 256的输入，经过9 X 9，2，32 八个卷积层的叠加，得到32个二维表示结构的胶囊，又或者说32 X 6 X 6个胶囊输出，而每个输出的维度是8D；

    不同于卷积层的操作的计算特点：将实体的部分拼接在一起形成一个相似的整体。

- 第三层：Digit胶囊层。每个8D的胶囊输出，经过仿射变换，得到16D的胶囊输出，而经过动态路由选择，优化父子胶囊之间的**连接程度**。

    作用：连接初级胶囊层，获取数字胶囊层。

注意：胶囊的定义，取决于其形式，而不是其获得的途径。比如说，初级胶囊层的胶囊是通过8个卷积层计算，从每个卷积层中抽取一张特征图叠加在一起形成一个8D输出的胶囊结构。而数字胶囊层16D输出的胶囊是通过仿射变换得到并经过动态路由优化而来。因此重要的是形式，而不是计算过程。**其中，初级胶囊层中的32X8=256张特征图，对应的256个卷积核的权重是不一样的。那么抽象中的胶囊结构，它的意义是什么？**

#### 胶囊网络的核心概念和理念

- 单个的胶囊是大量特征图的分组，而得到的每一组即是一个胶囊。初级的胶囊结构即是如此，**虽然明确了胶囊，但是胶囊的界限并不明确或者说是宽泛**

    ![image-20210914111117021](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/2021年度/F 胶囊网络/胶囊网络.assets/image-20210914111117021.png)

    两种初级胶囊阶段的胶囊组织形式，其实定义边界并不明显，从初级胶囊到数字胶囊的操作对256个特征图的处理是不存在差异的。

    **不过，胶囊的形式，从全文来讲，其实就是原来特征图是二维输出，变成了三维输出，这一点是胶囊形式的不容置疑的特点**。

- 胶囊的多维特征所表示的意义：**一个被激活的胶囊的神经元的激活值表示一张图片中一个特定实体的不同属性，作者称之为instantiation parameters，比如，pose（位置、大小和方向）、变形、velocity, albedo, hue, texture速度，色调和纹理等。同时还有一个很特殊的属性——某个实体是否存在于图像中。**

    - 是否存在：根据包含instantiation parameters的向量的模长，表示该实体是否存在；

        **TODO：直观感觉，神经网络中，如果要输出一个非0的向量，基本不可能，只会很小，但是不会不存在，因此这一点的价值比较怀疑。**

    - 向量的方向表示实体的属性

        **TODO：这一点似乎说的不明确，方向是个需要坐标轴才能具体呈现的抽象概念。同时，属性是多个，所以到底是如何计算或者映射的？**

- 为了使输出数据处于0和1之间，或希望其代表一种概率，可以使用非线性函数。softmax或者sigmoid都可以。

    ![image-20210914124413025](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/2021年度/F 胶囊网络/胶囊网络.assets/image-20210914124413025.png)

    **todo: 为什么不使用sigmoid呢？**

- 动态路由：它解决了将部分分配到整体的问题，即将子胶囊分配到恰当的父胶囊的问题，算法中是通过不断**优化两层之间的耦合系数矩阵**实现的。

    For the higher levels of a visual system, this iterative process will be solving the problem of assigning parts to wholes

    **todo: 神经网络中的深层结构不也是由部分连接到深层吗？**

    - 最初，前一层的每一个output（共1152个output）通过一个总和为1的耦合系数矩阵，会被路由到all possible parents，但是output也会被缩放。

    - 对于每一个可能的parent J ，前一层的每个胶囊 I 会计算出一个对应与parent J 的预测向量（是前一层的胶囊I的输出和一个权重矩阵的相乘）。

    - 如果这个胶囊J的预测向量和前一层胶囊I的输出向量具有比较大的标量积，则会产生一个自上而下（自后向前）的反馈：**胶囊I对应胶囊J的耦合系数会因此增加，进而胶囊的预测向量和parent的输出向量之间的标量积会进一步增加。**

    这种一致性路由连接操作，比先前的路由形式——最大化池化更有效率。因为池化路由，允许一个网络层中的神经元忽略 **在一个局部池中** 所有除了最积极的特征检测器以外的其他的检测器。**因为池化会丢失掉，某个区域中，关于一个实体的准确位置的信息**。

    

- 耦合系数矩阵：耦合系数矩阵独立于每个样本，每个样本下的系数矩阵的元素数目是前后两层胶囊数目的乘积。

- 仿射变换矩阵：

- 前几层胶囊的生成：

    - 借鉴CNN。

        CNN网络使用已经学习到的特征检测器的副本（即重用每个特征图的内核），这样的操作，允许这些检测器，将图像中某个位置获得的关于卷积核权重的知识转移到另一个位置。**这一点已经被证明是对图像解释很有帮助的，因此胶囊网络并没有打算抗拒这一点，替换这一点。**

    - 适合胶囊网络的卷积层的设计。

        对于底层胶囊，位置信息会因为该胶囊是否被激活而被位置编码；对于高层胶囊，越来越多的位置信息会被rate-coded速率编码在一个高层胶囊的输出向量的real-valued components实值分量中。

        这种从位置编码到速率编码的转变，再加上更高级别的胶囊代表具有更多的自由度的更复杂的实体，这种转变过程，表明**胶囊的维度应该随着我们层次的提升而增加**。

- 胶囊网络和卷积网络的差别：（具体原因上面有解释）
    - 将CNN中的标量输出特征检测器替换为向量输出的特征检测器。
    - 用一致性路由替换最大化池化。路由即是指前两一层单元向后一层单元的选择、具有程度值的选择过程。

#### 参考资料

- Understanding Hinton’s Capsule Networks  https://pechyonkin.me/capsules-1/ 
- [塞尚·卡马乔](https://cezannec.github.io/) 胶囊网络 https://cezannec.github.io/Capsule_Networks/
- 如何看待Hinton的论文《Dynamic Routing Between Capsules》 https://www.zhihu.com/question/67287444
- Keras版本的实现，含其他版本的链接 https://github.com/XifengGuo/CapsNet-Keras

- 学习Hinton老爷子的胶囊网络，这有一篇历史回顾与深度解读 https://www.jiqizhixin.com/articles/2020-08-12-11



## 卷积网络

#### CNN里的空间不变性，空间信息

卷积神经网络里的空间信息是什么意思，为什么都说CNN提取越深的层，得到的语义信息越丰富，空间信息却越贫乏？

首先我们要知道什么是语义信息，什么是空间信息

00100
01100
00100
00100

我们假设这是一幅图片，上面的每个数字是一个像素，我们是不是能看出来这张图片的要表达的是一个'1' 呢

我们人用肉眼去观察图片，我们能判断出这是一只猫，或者是一只狗，一个人。

**这样的判断行为对于计算机来说就是语义信息，而上面像素的排列组合方式就是空间信息，它可以像上面这么排，也可以像001100011100001100001100 这样排。**

- 为什么说层越深，语义信息越丰富。

    计算机面对一张图片，是不知道这是什么东西的，卷积核通常来说就是滤波器，经过不断的卷积，用各种各样的卷积核，我们才可以提取出“眼睛”“耳朵”“手”，并且将它们组合在一起，我们才能判断出这是个人还是狗。

- 为什么说层越深，空间信息越贫乏。

    在卷积的过程中，特征图是不断变小的（如果不加padding），或者还有池化的操作，经过一系列操作之后我们提取出来的特征已经找不到它原来在图片中对应的位置了

- 为什么说CNN具有空间不变性。

    空间不变性就是对位置信息不敏感，比如一张图片有一只狗，这只狗站在图片的左上方或者右下方都不妨碍这张图片有一只狗，无论这只狗的位置在哪，最后都能被CNN提取出来。

参考文章：

- CNN里的空间不变性，空间信息 https://blog.csdn.net/SY_qqq/article/details/107412539

- 卷积空间不变性 https://blog.csdn.net/MrMrshumo/article/details/108089490

    这篇在文章中提到：空间不变形是包括 平移不变形和形状不变性；

    ![image-20210914162417792](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/2021年度/F 胶囊网络/胶囊网络.assets/image-20210914162417792.png)

    这里的示例是语义级别的信息，如果深入到像素，其实就是某个region中的（卷积层）像素值分布结构或者（最大池化层）最大值的位置。

    如果图像中因为光照等原因，对脸部区域的像素产生了干扰，那么以往学习到的像素值分布结构就不能识别到这里有一个人脸。**因此特征检测器检测到的数据分布结构应该比较温和，不能苛刻，否则不具有鲁棒性，因此CV领域的图像增强预处理方式很有必要。**

