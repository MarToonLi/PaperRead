激活函数总结

- 什么是激活函数
- 激活函数的分类，分类依据又是什么？
- 目前常用的激活函数有哪些？常用的原因，优势是什么？



### 什么是激活函数

首先数据的分布大多是非线性分布的，而神经网络的目的是训练模型参数以匹配数据，因而模型必须也具有一定的非线性；

如果模型是由全连接层和卷积层的连续重复使用构成，那么**模型拟合能力或者称为表达能力**将得不到提升，几乎能力相当于一层的卷积或者全连接层。

神经网络模型本质是为了表达一个函数，如果是一个线性函数是表达不了太多东西的，然而非线性函数却可以。远不仅如此，从理论上来讲，**90年代初的Universal approximator theorem 表示，对于两层的神经网络和一个sigmoid非线性激活函数，只要网络足够大，那么就可以通过合理设定参数矩阵来近似所有的连续函数和各种其他函数。**

总的来说就是，模型需要匹配/拟合数据的非线性分布，而一般的网络层无法赋予模型非线性，因而模型需要引入具有非线性特征的激活函数，从而使自身具有非线性。**非线性意味着模型强大拟合能力的基础**。

—— 沈佐伟（Zuowei SHEN）老师：“深度近似是通过多层简单函数的组合来近似一个函数，可以看作是一系列嵌套的特征提取器。深度学习网络的关键思想是将组合层转换为可通过学习过程调整的可调参数层，从而实现对**输入数据的良好近似**”。

>1 神经网络有什么理论支持？ https://zhuanlan.zhihu.com/p/27609166
>
>2 神经网络为什么可以（理论上）拟合任何函数？ https://www.zhihu.com/question/268384579/answer/540721803

### 激活函数的分类及其依据

- 右饱和：
    当x趋向于正无穷时，函数的导数趋近于0，此时称为右饱和。
- 左饱和：
    当x趋向于负无穷时，函数的导数趋近于0，此时称为左饱和。
- 饱和函数和非饱和函数：
    当一个函数**既满足右饱和，又满足左饱和（分类依据）**，则称为饱和函数，否则称为非饱和函数。

常用的饱和激活函数和非饱和激活函数：
饱和激活函数有如Sigmoid和tanh，非饱和激活函数有ReLU；

优劣对比：**相较于饱和激活函数，非饱和激活函数可以解决“梯度消失”的问题，加快收敛**。

### 梯度消失和梯度爆炸的原因和解决思路

首先，梯度消失和梯度爆炸的问题是基于SIgmoid函数进行的讨论。

原因是深度学习研究初期，生物神经元似乎是Sigmoid型函数进行活动的，因此研究人员在很长的一段时间内都坚持使用SIgmoid函数。但事实证明，Relu激活函数通常在ANN（人工神经网络）上工作得更好。

其次，梯度消失和梯度爆炸的问题的原因由三方面构成（而这三条可以通过公式推导得出）：

- 神经网络的多层特征；有时层数会极大；（**本质上是因为梯度反向传播中的连乘效应**）
- 激活函数的导数特点：不同的x值，其导数值不同；小于1的导数和大于1的导数本身没有任何问题，**但是多层网络这一特点**会将模型在反向传播时，越靠后计算导数的网络层的权重的导数值 **呈现出前层导数值越小，后层导数值越小；前层导数越大，后层导数越大的现象，即梯度消失和梯度爆炸**
- 网络层的权重值。这一点本身没有问题，但是在激活函数和神经网络的特点之下，会一定程度地 **推动** 梯度消失和梯度爆炸的问题。

具体地，

- 假设的场景：激活函数全部时sigmoid函数；输入只有一个特征，各层的参数只有一个，且不含偏置项。

    <img src="/home/cold/PaperReadFastly/PaperRead/论文阅读列表/激活函数/激活函数总结.assets/image-20210906165311798.png" alt="image-20210906165311798" style="zoom: 67%;" />

    

- 梯度消失问题分析：一般模型的权重初始化，会以每层为基本单位，使用一个均值为0方差为1的高斯分布。因此每层的权重通常会满足 **权重值的范数$|W_i|$  小于1**；其二，当输出值$Z_n$值在计算过程中偏大时，只要小于1，那么最终得到的梯度值会越小。尤其是当输出值过大或者过小时，导数值会越来越接近0，进而梯度消失问题会愈加严重。
- 梯度爆炸问题分析：主要是权重大于1的累积而成，因为梯度值（sigmoid的导数）最大为0.25。（**因此，对于梯度爆炸，它不是激活函数换一换就可以解决的问题。**）

解决思路有（从权重和激活函数两方面考虑，具体是权重初始化、激活函数的选择和**训练过程中权重值和梯度值的控制**）：

- 适合的参数初始化方法；
- 非饱和的激活函数；
- 批归一化；
- **梯度截断**；
- 更快的优化器，避免过多的训练次数；
- LSTM的结构设计也可以改善RNN中的梯度消失问题；

>梯度消失和梯度爆炸原因及其解决方案 https://blog.csdn.net/junjun150013652/article/details/81274958
>
>神经网络训练中的梯度消失与梯度爆炸 https://zhuanlan.zhihu.com/p/25631496



### 具体的激活函数

所谓的非线性，是一阶导数不为常数。

#### 1 Sigmoid函数

**起源：** 其也称为逻辑logistic函数，因为sigmoid函数是从logistic回归中推理得到的；

**数值特征：**

- 取值范围，在（0，1）之间，可以将网络的输出映射在这一范围；
- sigmoid函数的导数，1-函数值的结果成以函数值；导数的最大值为0.25，最小值为0；
- 形式，1+e的负x方的结果的倒数；（因为涉及指数运算和除法运算，因此计算速度比较慢）

优点：

- 整体平滑，易于求导；

缺点：

- 计算量大，计算速度偏慢：因为正向和反向传播中都包含幂运算和除法运算；（除法会涉及inf和0，nan的问题）
- 导数取值范围介于0到0.25之间，小于1：因为神经网络反向传播中的链式反应，容易出现梯度消失的问题；
- 函数取值范围不是0均值，zero-centered：使得后一层的神经元得到上一层输出的非0均值数据作为输入，而随着网路的加深，会使得每层的数据分布与原始数据分布不匹配的问题，直接影响拟合效果。

#### 2 tanh函数

**起源**：又名，双曲正切函数，英文hyperbolic Tangent，与sigmoid函数，均被归类为饱和激活函数。

数值特征：

- 取值范围：从sigmoid的（0，1）变成（-1，1），可以看成sigmoid函数向下的平移和拉伸后的结果。

- 形式：（同样涉及大量的幂运算和除法运算）

    <img src="/home/cold/PaperReadFastly/PaperRead/论文阅读列表/激活函数/激活函数总结.assets/image-20210906184216280.png" alt="image-20210906184216280" style="zoom:80%;" />

- 导数取值范围：0和1之间，

优点（相较于sigmoid）：

- 输出范围是-1到1，因此解决了sigmoid函数不是zero-centered输出的问题；
- tanh的导数范围在0和1之间，相比于sigmoid的0到0.25，梯度消失问题会得到轻度缓解。

缺点：

- 梯度消失问题得到缓解，但是依旧存在；
- 幂计算和除法计算的存在，计算量增加，计算速度相对比较慢。

#### 3 ReLU函数

起源：称为修正线性单元函数，rectified Linear Unit。

数值特征：

- 正数时：
    - 形式：由一个简单的导数为1的线性函数构成，取值范围：从0到正无穷；

    - 导数，取值为1：梯度消失的强度就完全取决于权重值的连乘。

    - 优点：

        - 相较于sigmoid函数，没有类似幂运算和除法运算的复杂运算，提高了计算速度；

        - 将梯度消失中的激活函数导数部分的因素**完全解决掉了**，但是深度学习中的梯度消失和梯度爆炸问题仍然存在（主要由每层的权重连乘决定）。

            如此，从一定程度上加快了模型的收敛速度，比较稳定。

    - 缺点：

        - 因为梯度爆炸和梯度消失问题中的导数因素被排除，权重引起的两种问题成为主要因素；**缓解途径是：梯度裁剪和控制权值，比如batch normal**。

- 负数时：

    - 形式：为0：
    - 导数：为0；

    （**因为深度学习的目标是，从大量的，具有错综复杂的关系的样本数据中，找到关键特征。换句话说，是一个样本数据从密集矩阵转化为稀疏矩阵的过程，保留下关键特征，去除噪声特征，如此模型也就有了鲁棒性！**）

    - 优点：
        - 小于0部分，将导数置为0的操作，就相当于一个**去除噪声特征，获取稀疏矩阵的操作**，同时因为在训练过程中，这种稀疏化操作是动态调节的，因此会保证矩阵得到最优的稀疏比例；
    - 缺点：
        - 成也萧何，败也萧何；梯度置于0的操作，就等价于屏蔽该特征，会导致模型无法学习到有效的特征。如果学习率设置比较大，就可能会导致网络的大部分神经元处于dead状态。因此，使用relu函数，学习率不能过大，容易丢失有效特征。

#### 4 Leaky ReLU, PReLU（Parametric Relu）, RReLU（Random ReLU）

为了防止模型的dead情况，研究人员对于负数部分进行了修正。

- Leaky ReLU，指定斜率，导数值。

- PReLU，导数值作为可学习的参数；

- RReLU，**导数值在训练中从一个均匀分布区间中随机变化（亮点）**，不属于学习型，但是之后的测试就变成了固定的；

    ![image-20210906191832030](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/激活函数/激活函数总结.assets/image-20210906191832030.png)

#### 5 SoftPlus

Softplus函数是Logistic-Sigmoid函数原函数。加了1是为了保证非负性。Softplus可以看作是**强制非负校正函数max(0,x)平滑版本**

<img src="/home/cold/PaperReadFastly/PaperRead/论文阅读列表/激活函数/激活函数总结.assets/aHR0cDovL2ltYWdlLm1hbWljb2RlLmNvbS9pbmZvLzIwMTUwNi8yMDE4MDkyMjA4MjA1NjgwNjc3NS5wbmc" alt="技术分享" style="zoom:50%;" />



> 激活函数总结（持续更新） https://zhuanlan.zhihu.com/p/73214810	

