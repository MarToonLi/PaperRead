### 参考的模板代码

#### 1 torch.nn.DataParallel

##### 1.1 指定使用的显卡编号

```text
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,3' # 此指定需要置于模型加载到GPU上之前。
# 建议作为parser中的一个参数。
import argparse
parser = argparse.ArgumentParser(description='PyTorch Example')
parser.add_argument('--disable-cuda', action='store_true',
                    help='Disable CUDA')
parser.add_argument('--devices',defult='0', help='Disable CUDA')                 
args = parser.parse_args()

# todo: 等待补充，根据devices的数目和首个数字，判断是否GPU禁用，以及是否多卡运行。

if not args.disable_cuda and torch.cuda.is_available():
    args.device = torch.device('cuda')
else:
    args.device = torch.device('cpu')
```

##### 1.2 模型的并行化

```text
model = nn.DataParallel(model)
model = model.cuda()
```

##### 1.3 数据的并行化

```text
inputs = inputs.cuda()
labels = labels.cuda()
```

##### 1.4 平衡显存使用不平衡的问题

参考当前目录下的BalancedDataParallel.py文本。其中因为0号GPU还需要承载一些数据的整合操作，因此0号GPU需要被分配到的样本低于平均水平。

这个 `BalancedDataParallel` 类使用起来和 `DataParallel` 类似, 下面是一个示例代码:

```text
my_net = MyNet()
my_net = BalancedDataParallel(gpu0_bsz // acc_grad, my_net, dim=0).cuda()
```

跑的大小是2+3+3=8, 于是你可以设置下面的这样的参数

```text
batch_szie = 8
gpu0_bsz = 2
acc_grad = 1
my_net = MyNet()
my_net = BalancedDataParallel(gpu0_bsz // acc_grad, my_net, dim=0).cuda()
```

这个时候突然想跑个batch size是16的怎么办呢, 那就是4+6+6=16了, 这样设置累积梯度为2就行了:

```text
batch_szie = 16
gpu0_bsz = 4
acc_grad = 2
my_net = MyNet()
my_net = BalancedDataParallel(gpu0_bsz // acc_grad, my_net, dim=0).cuda()
```

##### 1.5 运行更快的一种并行方式

pytorch的官网建议使用`DistributedDataParallel`来代替`DataParallel`, 据说是因为`DistributedDataParallel`比`DataParallel`运行的更快, 然后显存分屏的更加均衡。

```text
torch.distributed.init_process_group(backend='nccl', init_method='tcp://localhost:23456', rank=0, world_size=1)
```

第一个参数是pytorch支持的通讯后端, 后面会继续介绍, 但是这里单机多卡, 这个就是走走过场. 

第二个参数是各个机器之间通讯的方式, 后面会介绍, 这里是单机多卡, 设置成**localhost**就行了, 后面的端口自己找一个空着没用的就行了. 

**rank是标识主机和从机的, 这里就一个主机, 设置成0就行了**. 

**world_size是标识使用几个主机, 这里就一个主机, 设置成1就行了,** 设置多了代码不允许.

或者一种专门面向单机多卡的定义声明方式：

```text
torch.distributed.init_process_group(backend="nccl")
model = DistributedDataParallel(model) # device_ids will include all GPU devices by default
```

但是这里需要注意的是, 如果使用这句代码, 直接在pycharm或者别的编辑器中,是没法正常运行的, 因为这个需要在shell的命令行中运行, 如果想要正确执行这段代码。

```text
python -m torch.distributed.launch main.py
```

*注: \*这里如果使用了argparse, 一定要在参数里面加上\*`*--local_rank\*`*, 否则运行还是会出错的**

参考文章：https://zhuanlan.zhihu.com/p/86441879
