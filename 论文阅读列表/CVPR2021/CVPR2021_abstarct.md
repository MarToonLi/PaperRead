### 《GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields》

​		通过**将场景表示为合成生成神经特征场**，能够提取出场景中的物体，并对物体进行平移旋转等操作。**而其中对物体的提取和transform操作是作者强调的内容创造可控的具象表现**。其中很重要的一个设计是，valume render操作从原来的直接输出一个RGB值，变成先输出一个多维特征向量，向量经过卷积网络后再输出一个RGB值，虽然过程多了，但是运行速度似乎更快一些。 

- 神经特征场：理解成图像经过神经网络得到的特征表示。
- 内置控制：

摘要：

虽然现有的深度生成模型允许以高精度逼真的进行图像合成。但是这却远远不够：**内容创造同样需要可控**。虽然近几年的研究都在研究如何disentangle underlying factors of variation in the data理顺数据变化中的潜在因素，但是它们的操作基本基于2D，而忽视了我们的世界是三维的。

同时，只有少数研究考虑了**场景的构图性质**。

**作者的关键假设是：将合成3D场景表示合并到生成模型中，这个假设将会使得图片合成更加可控。**

<u>将场景scence表示为合成生成compositional generative的神经特征场**feature fields**，允许我们从背景中分离disentangle出一个或者多个对象以及单个对象的形状和外观，同时无任何额外的监督即可从非结构化和未使用的图像集合image collections中学习到</u>。

如果将这种场景表示与神经渲染管道**neural rendering** pipeline 相结合，可以生成快速，逼真realistic的图像合成模型。而实验表明，模型能够分离出单独的物体，同时translate平移和旋转他们，以及更改相机的pose。



引言

现代计算机图形技术水平已经在工业由所应用，但是它们的硬件十分昂贵，需要大量的人力来创造和安排creation and arrangement 3D content 。

生成一张逼真的2D图片并不是合成模型应用的唯一方面，**我们希望生成过程能够以一种简单一致的方式in a simple and consistent manner，变得可控。**

以往的工作研究了如何以非具体的监督方式从数据中获取到disentangled representations分离的表示。disentanglement分离的定义存在差异，但是通常指代能够在不改变其他属性的情况下控制该兴趣的特征，比如物体形状、大小和pose姿势。

**而目前大多数的方法，即没有考虑到场景中的合成性质，也忽视了物体的三维性质。而这样的现状研究出的结果将会引起纠缠表示，同时虽然控制机制不是内置的，但是却需要在后验的潜在空间中发现**。比如下图。

然而，上述的三维性质和合成性质，对实际应用非常重要，比如，电影制作中复杂的物体轨迹需要以一致的方式得以生成。

![image-20210910131501603](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910131501603.png)



Fig2：这将导致**更一致**的图像合成结果，例如，请注意，与我们的方法相比，

- **在2D中操作时，平移一个对象可能会改变另一个对象，但是proposed方法不会**（图2a和2b）。

- 它还允许我们执行复杂的操作，如循环平移（图2c）或**在测试时添加更多对**象（图2d）

    这两种方法都是在无监督的情况下对两个目标场景的未经处理的原始图像集进行训练。

当然了，也有几个工作研究了如何将3D表示 **直接** 合并到生成式模型中。比如，voxels [32,63,64], primitives [46], or radiance fields [77]。虽然这些方法允许通过**内置控制**获得令人印象深刻的结果，但它们大多局限于**单对象场景**，对于更高分辨率和更复杂、更真实的图像（例如，**对象不在中心或背景杂乱的场景**），结果不太一致。

**Contribution**

- 引入新方法GIRAFFE，允许在从原始的、非结构化的图片集合的训练过程中，以一种可控的、逼真的方式生成场景。

    - 将一个合成的3D场景表示直接合并到生成式模型中，这样会使得图片合成更加可控；
    - 将显示的3D表示和一个神经渲染管道结果合并，这会实现更快的推理和更加逼真的图片。

    因此，将场景表示为合成的生成式神经特征场。

    ![image-20210910130925865](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910130925865.png)

    图1：概述。我们将场景表示为合成生成的神经特征场。

    具体过程：

    - 对于**随机采样的相机**，我们基于**单个特征场**对场景的特征图像进行**体积渲染**。
    - **2D神经渲染网络**将特征图像转换为RGB图像。

    具体效果：

    - 虽然只针对原始图像采集进行培训，但在测试时，我们能够**控制wrt的图像形成过程。摄影机姿势、对象姿势以及对象的形状和外观**。
    - 此外，我们的模型**超越了训练数据**，例如，我们可以合成具有比训练图像中更多对象的场景。请注意，为了清晰起见，我们将体积可视化为彩色，而不是特征（TODO：不懂）。

- 过程：我们将**场景体积渲染**volume render为相对低分辨率的特征图像，以节省时间和计算量。**神经渲染器neural renderer**处理这些特征图像并输出最终渲染。

- 效果：通过这种方式，我们的方法可以获得高质量的图像，**并可扩展到真实场景scales to real-world scenes**。我们发现，当对原始非结构化图像集合进行训练时，我们的方法允许对**单个对象和多对象场景进行可控的图像合成**。（多物体+真实场景+高分辨率+基于原始非结构化数据）



#### 2 Related Work

**基于GAN的图片合成：**

为了获取对合成过程更好的控制，许多工作研究了 **在没有明确监督的情况下**分离变异因素，而研究的方式无非是修改训练主题，要么是网络结构，又或者是研究investigate latent spaces of well-engineered and pre-trained generative models**精心设计和预先训练的生成模型的潜在空间**。

然而，这些工作，并没有**对场景的合成特点进行明确的建模**。

有一些工作也因此研究了合成过程如何在物体级别达到可控。 虽然实现了逼真的结果，但是上面提及的所有的work对图片信息的建模都是在2D层次，忽视了三维的机构。

因此，**这部分强调的是，对场景的合成特点的建模**

**Implicit Functions:**

使用隐式函数表示三维集合体在基于学习的3D重建中得到了广泛的应用，并且扩展到了场景级别的重建。

为了克服三维监控的需要，先前的几个工作提出不同的渲染技术，【61】提出了神经辐射场，它将一个隐含神经模型和体积渲染结合，用于复杂场景的新型视图合成。

基于该模型的表现力expressiveness，我们使用了NeRFs的生成式变体，作为本文物体级别的表示。**与我们的方法不同的是**，所讨论的工作需要**多视图图像**，以摄像机姿势作为监控，每个场景训练一个网络，并且不能生成新的场景。**相反**，我们从**非结构化图像**集合中学习生成模型，该模型允许**生成场景的可控照片级**真实感图像合成。

**3D-Aware Image Synthesis 三维感知图像合成:**

有一些工作研究了三维表示如何被当作归纳偏倚合并到生成式网络中。尽管许多方法使用额外的监控，但是我们专注于像我们的方法一样对原始图像收集进行训练的努力。

【32】学习了基于oxel的表示，通过不同的渲染技术；实现了三维可控，但由于立方体内存增长导致的体素分辨率有限，因此会出现伪影。

【63，64】提出了**体素化特征网格表示**，**通过重塑操作将其渲染为二维**。在取得令人印象深刻的结果的同时，**训练变得不太稳定**，结果也不太一致，**无法获得更高的分辨率**。

【46】将抽象特征与基本体和可微分渲染结合使用。在**处理多对象场景时，它们需要以纯背景图像的形式进行额外的监控，这对于真实场景来说是很难获得的**。

【77】提出了生成性神经辐射场（GRAF）。虽然在高分辨率下实现可控图像合成，但这种表示仅限于单对象场景，在更复杂的真实图像上效果会降低。

相比之下，我们将合成3D场景结构合并到生成模型中，以便它自然地处理多对象场景。此外，通过集成神经渲染管道，我们的模型可以扩展到更复杂的真实数据。

#### 3 Model

神经辐射场，是一个连续性的函数，它将**三维点坐标和一个视角方向**映射为一个体积密度值和一个RGB颜色值（三维），而【61，82】指出如果训练函数是一个神经网络，那么低纬度的两个输入必须被映射成高纬度特征才能表示复杂的信号。

其中有个特殊的设计——引入参数L，以元素相乘的方式，融入到各个角度中。

参数的意义在于：它通过引入一个归纳偏差，在规范方向上学习三维形状的表示，如果学习不大，那这个值将会是任意的。

![image-20210910163730156](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910163730156.png)



![image-20210910163621351](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910163621351.png)

而神经辐射场的模型，其实是一个多层感知机。

**生成式神经特征场**：

在神经辐射场的基础上，增加了两个方面：增加了潜在空间和将三维颜色输出转化为多维特征；

![image-20210910171715893](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910171715893.png)

- 潜在空间，受到【77】的GRAF的启发，认为虽然MLP能够拟合单个场景中的多个姿势图像，但是认为表达能力还是比较受限，因此使用了潜在空间，具体的代码，就是两个正态分布的矩阵，经过处理后融入到原始数据中。
- 多维特征，目的其实不明。但是可以明确三个值不足以满足需求。

**Object Representation**

现有的一些模型的关键局限性在于，整个场景完全使用一个模型来表示。**而正如我们所感兴趣的是解开场景中的不同实体，因此我们需要对每个对象（包含背景）的pose，形状和外貌apperance进行控制。**因此，作者使用一个独立的特征场和一个仿射变换矩阵来表示每个物体，具体的是由缩放参数和平移参数以及旋转矩阵构成。

![image-20210911152628912](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210911152628912.png)

##### 3.2. Scene Compositions

##### 3.3. Scene Rendering

**3D Volume Rendering:**

volume render 就是一种渲染绘制三维数据的技术。[Volume Render](http://www.visitusers.org/index.php?title=Volume_Rendering#Ray_casting)通常用来绘制**几何图形**难以表现的流体、云、火焰、烟雾等效果。

**2D Neural Rendering:**

以往的工作希望得到一个RDB值，而当前工作得到一个多维特征向量，而多维特征向量协同输入图像，经过一个二维的卷积神经网络得到一个RGB值。**因此渲染操作，考虑了二维空间结构。**

GIRAFFE的整体结构

![image-20210911154530791](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210911154530791.png)

<img src="/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210911154640253.png" alt="image-20210911154640253" style="zoom:67%;" />



### 《Back to the Feature: Learning Robust Camera Localization from Pixels to Pose》

#### 摘要

动机：已知场景中的相机姿势估计任务是一个3D几何性质的任务。**许多模型算法**从输入图像中**回归**精确的几何量，如姿势或3D点。

**但是这些几何量模型算法，要么无法泛化到新的视角，要么将自身模型的参数绑定到特定的场景中。**

本文，我们将回归特征：**作者认为深度网络应该专注于学习鲁棒和不变的视觉特征，而几何估计应该留给有原则的算法。**

> **todo：是否可以认为一个模型，应该分为两个部分，backbone学习良好的特征表示，head面向具体的应用场景。**

为此，提出了一个面向未知场景的神经网络，以从一张图片或者一个3D模型中估计出一个准确的6自由度姿势（**这里的姿势，具体来说是由六个变量和他们的空间结构来阐释的。**）。



方法介绍：基于多尺度深度特征，将相机位置转换对度量（**多个变量值**）的学习。

方法的性质：

- 端到端学习使得模型学习到强数据先验知识；
- 将模型参数和场景几何分离开，体现出巨大的场景泛化性。

模型的能力：

- 该系统可以在给定粗姿态先验的大环境下进行定位
- 也可以通过联合细化关键点和姿态，以较小的开销**提高稀疏特征匹配的精度**

#### **引言**

问题介绍：视觉定位是在已知场景中估计给定图像的摄像机位置和方向的问题。解决这个问题是实现自动驾驶汽车等真正自主机器人的关键一步，也是增强和虚拟现实系统的先决条件。

当前研究方式：普遍依赖于2D的像素位置和3D点坐标。它通过使用一个PnP求解器以估计相机的pose。

以往的2D和3D协同的方法一般是通过匹配局部图片特征；而目前的定位系统可以解决具有

最近的研究方法可以处理复杂几何和外观随时间变化的大场景；

神经网络用于提取这样的特征，匹配几何量和外观，同时过滤离群值的对应关系。

---

替代方案：以一种端到端的方式训练特征以匹配pipeline是具有挑战性的，同时是不稳定的，因为这种方式的复杂度阻碍了梯度传播。**替代方案是：训练卷积网络以回归几何量，比如相机姿势或者每个像素对应的3D场景坐标。**

替代方案有问题：

- 绝对姿势和坐标回归是面向特定场景的，同时需要被训练以适应新的场景。**泛化到新的视角条件，光亮或者场景复杂程度，都是具有挑战性，对于提出的替代方案。**
- 无论绝对姿势回归还是相对姿势回归都限制了准确率，同时不能泛化到新的视角。**虽然相对于一组参考图像回归姿势[5,24,43,99]在理论上是场景不可知的**，但据我们所知，到目前为止，还没有显示出在**姿势精度没有显著下降的情况下**对不同场景进行概括[78,99]。

- 仅仅从图片中预测相机姿势或者3D几何量，**阻碍了现有端到端回归方法的泛化**。事实上，这些几何量是现成的。**而**，
    - 姿态先验可以通过图像检索或GPS等传感器获得。
    - 同时，3D场景几何体通常作为生成训练姿势的3D重建系统的副产品提供，例如，通过运动或SLAM生成结构。

解决方案的启发点：

- 直接图片对齐
- 孤立点剔除的学习图像表示法

基于以上，认为，端到端的视觉定位算法应该专注于特征表示的学习；**与其将模型容量和数据用于学习基本几何关系或对3D地图进行编码，还不如依赖于理解良好的几何原理，并学习对外观和结构变化的鲁棒性。**

> 那么哪些行为，促使了这种泛化性？

**呈现方案：PixLoc**，基于CNN提取出的特征，通过将图片与一个具体地3D场景模型对其，以定位一个图片。

依赖于经典的几何优化方法，神经网络不需要自己学习姿态回归，但是仅仅需要提取合适的特征，以促使算法准确并具有场景不可知性。

对于一个给定的从图像检索中获得的初始姿态，我们的架构会得到一个简单的定位架构，与现有的复杂方法相匹敌。PixLoc还可以将任何**现有方法估计的姿势**细化为**轻量级的后处理步骤**。

#### 相关工作

**准确的视觉定位：**通常依赖于估计二维像素位置和三维场景坐标之间的对应关系。

- 而这些传统的方法：
    - 检测、描述和匹配局部特征；
    - 保留且显示系数的三维的环境表示；
    - 有时也会使用图像检索，以缩放到大场景；

- 一些新的组件也获得了成功，但是通常因为系统的复杂性，而相互独立和非端到端。
- 为此，我们提出一个简单的替代方案以实现特征匹配.同时实现稳定的端到端训练；同时可以学习到更多的先验知识。同时具有一定的可解释性。

**定位任务的端到端学习方式：**

- 传统方法

    - 通过从输入图像**回归**到绝对姿势[35,37,61,68,92]或三维场景坐标[9,13,16,17,82]，**将场景编码到深度网络中**。

    - 姿势回归缺乏几何约束，因此限制了对新的视角和外观的泛化，纵使坐标回归的鲁棒性比较号。

        **由于网络容量有限[11,84]，两者都不能很好地扩展，而且每个新场景都需要昂贵的再培训或调整[16,17]**

- 另外的一些方法。

    经常地通过一个恢复步骤之后，回归得到一个相机姿态，这个姿态与一个或多个训练图片相关，

    - 不存储场景的几何关系，因此具有场景无关性。但是准确率低一些；
    - 与我们更接近的是，SANet[95]通过从输入的3D点云回归3D坐标的方式，实现将场景表示从网络中移除。**关键的是，所有性能最好的可学习方法至少都是针对每个数据集（如果不是针对每个场景）进行培训的，并且仅限于小型环境[37,82]**

- 本文中，首次提出一种端到端学习模型，可以泛化到其他的场景，包括室外时内，同时性能呢各可以与一些复杂架构相媲美，这一点多亏了differentiable pose solver.

**学习相机姿态优化：**可以通过**将优化器展开固定数量的步骤**[21,52,54,85,93,94]，计算隐式导数[13,15,18,34,70]，或精心设计损失来模拟优化步骤[90,91]来解决。

- 其中一些公式优化稀疏点上的重投影误差，而另一些公式使用直接目标进行（半）密集图像对齐。后者因其简单性和准确性而具有吸引力，但通常不能很好地扩展。

- 与经典的同类产品[26,38]一样，它们也受到小范围收敛的影响，仅限于帧跟踪。相比之下，PixLoc是针对宽基线交叉条件相机姿态估计（从稀疏测量中）进行明确训练的（图2）。通过专注于学习好的特征，它表现出良好的泛化能力，同时学习形成优化目标的合理数据先验。





### 《FrameExit: Conditional Early Exiting for Efficient Video Recognition》

#### 摘要

**帧退出：条件性提前退出以实现搞笑视频识别**  高通人工智能研究公司

尽管现有的研究专注于选择从salient突出明显的帧序列中选择一个子集以降低计算成本，但是作者提出通过一个简单的结合了条件性退出的采样策略以促进搞笑识别（**前者是提升计算性能，后者是提升准确率，正交的性质。**）。

- 模型能够根据视频的复杂程序自动选择合适数量的帧序列。**而实现的方法是：通过级联的门模块，自动决定推理过程中足够信赖的最早的点。**
- 我们向这些门模块生成**实时监控信号**，以在精度和计算成本之间进行动态权衡。

- 数据集：ActivityNet1.3 and mini-kinetics

#### 引言

视频的内容的大量的增多，引起了对高效识别行为和事件的需求。尽管在**识别复杂和极端行为方面**表现出了一流的性能，但**高数据量、计算需求和延迟需求**限制了最先进的视频识别模型在**资源受限设备**上的应用。



### 《Learning Salient Boundary Feature for Anchor-free Temporal Action Localization》

#### 摘要

时序行为定位任务是一项重要的，但是具有挑战性的问题，在视频理解方面。

一般而言，这类任务旨在推断长的未剪辑的视频中，每个动作实体的动作类型以及开始帧和结束帧的位置。

- 以前大多数的模型通过使用预定义的锚和大量的行为，取得了好的结果；

    但是这些模型容易受到**大量输出**、**频繁的位置调整和不同锚对应的大小的调整。**，

- 而无锚的方法相对比较轻便、避免了大量的超参数，却少有人关注。

本文，首次提出纯无锚时序定位方法，高效又快捷。

模型的主要结构：

- 端到端的可训练的基础预测器；
- 一个基于显著性的**细化**模块，通过**新颖的边界池化**为每个提案收集更多有价值的边界特征；
- 几个一致性的约束，确保模型能够基于任意的proposals找到准确的边界。

大量的实验表明，我们的方法在**THUMOS14上比所有基于锚和动作引导的方法都有显著的优势**，实现了最先进的结果，并且在**ActivityNet v1.3上具有可比性**。



#### 引言

相对于行为识别，TAL时序行为定位，不仅仅对每个视频中的每个激活实例进行分类，而且寻找他们的准确的时序位置。



### 《No frame left behind: Full Video Action Recognition》

### 摘要

对于识别行为，并不是所有的视频帧都是具有同等的信息量。如果一个行为持续超过百帧，那么训练模型在这些帧上，计算量将会不灵便。

常见的启发式做法：均匀采样得到少量的视频帧，同时使用这些帧是被行为。

我们提出一种full video 行为识别的方法，**考虑到所有的视频帧。**

为了是计算容易处理，

- 我们首先根据与分类任务的相似性沿时间维度对所有帧激活进行聚类；
- 时序的角度融合簇中的所有帧，变成一个较小数量的表示；

模型易处理同时计算搞笑，**因为它依赖于时间局部聚类和特征空间中的快速汉明距离**

数据集：UCF101, HMDB51, Breakfast, and Something-Something V1 and V2；与现有的**启发式框架抽样方法**相比，效果更好。



> **是否，一个问题可以视为一个很长的序列，对于这个序列的特征表示的学习方法：**
>
> - 可以将序列中的所有元素都考虑；
>     - 每个元素的权重一致；
>     - 每个元素，被珍视的权重，不同；
> - 采样出序列的元素，得到一个子序列。（计算高效）

### 《Removing the Background by Adding the Background: Towards Background Robust Self-supervised Video Representation Learning》

#### 摘要

自监督学习很有潜力，但是在视频理解方面，一些研究方法的预测结果，十分容易受到背景的影响，也就是对背景具有依赖性。

本文方法：

- 首先选择一张静态图片，将其加到其他的每张图片上，以构建一个分散的distracting视频样本。
- 然后，我们强制模型**拉近**干扰视频的特征和原始视频的特征，**使得模型能够明确地限制以抵抗背景的影响，专注于更多motion变化。**

数据集：UCF101 and HMDB51,MoCo
