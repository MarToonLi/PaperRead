### 《GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields》

​		通过**将场景表示为合成生成神经特征场**，能够提取出场景中的物体，并对物体进行平移旋转等操作。**而其中对物体的提取和transform操作是作者强调的内容创造可控的具象表现**。其中很重要的一个设计是，valume render操作从原来的直接输出一个RGB值，变成先输出一个多维特征向量，向量经过卷积网络后再输出一个RGB值，虽然过程多了，但是运行速度似乎更快一些。 

- 神经特征场：理解成图像经过神经网络得到的特征表示。
- 内置控制：

摘要：

虽然现有的深度生成模型允许以高精度逼真的进行图像合成。但是这却远远不够：**内容创造同样需要可控**。虽然近几年的研究都在研究如何disentangle underlying factors of variation in the data理顺数据变化中的潜在因素，但是它们的操作基本基于2D，而忽视了我们的世界是三维的。

同时，只有少数研究考虑了**场景的构图性质**。

**作者的关键假设是：将合成3D场景表示合并到生成模型中，这个假设将会使得图片合成更加可控。**

<u>将场景scence表示为合成生成compositional generative的神经特征场**feature fields**，允许我们从背景中分离disentangle出一个或者多个对象以及单个对象的形状和外观，同时无任何额外的监督即可从非结构化和未使用的图像集合image collections中学习到</u>。

如果将这种场景表示与神经渲染管道**neural rendering** pipeline 相结合，可以生成快速，逼真realistic的图像合成模型。而实验表明，模型能够分离出单独的物体，同时translate平移和旋转他们，以及更改相机的pose。



引言

现代计算机图形技术水平已经在工业由所应用，但是它们的硬件十分昂贵，需要大量的人力来创造和安排creation and arrangement 3D content 。

生成一张逼真的2D图片并不是合成模型应用的唯一方面，**我们希望生成过程能够以一种简单一致的方式in a simple and consistent manner，变得可控。**

以往的工作研究了如何以非具体的监督方式从数据中获取到disentangled representations分离的表示。disentanglement分离的定义存在差异，但是通常指代能够在不改变其他属性的情况下控制该兴趣的特征，比如物体形状、大小和pose姿势。

**而目前大多数的方法，即没有考虑到场景中的合成性质，也忽视了物体的三维性质。而这样的现状研究出的结果将会引起纠缠表示，同时虽然控制机制不是内置的，但是却需要在后验的潜在空间中发现**。比如下图。

然而，上述的三维性质和合成性质，对实际应用非常重要，比如，电影制作中复杂的物体轨迹需要以一致的方式得以生成。

![image-20210910131501603](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910131501603.png)



Fig2：这将导致**更一致**的图像合成结果，例如，请注意，与我们的方法相比，

- **在2D中操作时，平移一个对象可能会改变另一个对象，但是proposed方法不会**（图2a和2b）。

- 它还允许我们执行复杂的操作，如循环平移（图2c）或**在测试时添加更多对**象（图2d）

    这两种方法都是在无监督的情况下对两个目标场景的未经处理的原始图像集进行训练。

当然了，也有几个工作研究了如何将3D表示 **直接** 合并到生成式模型中。比如，voxels [32,63,64], primitives [46], or radiance fields [77]。虽然这些方法允许通过**内置控制**获得令人印象深刻的结果，但它们大多局限于**单对象场景**，对于更高分辨率和更复杂、更真实的图像（例如，**对象不在中心或背景杂乱的场景**），结果不太一致。

**Contribution**

- 引入新方法GIRAFFE，允许在从原始的、非结构化的图片集合的训练过程中，以一种可控的、逼真的方式生成场景。

    - 将一个合成的3D场景表示直接合并到生成式模型中，这样会使得图片合成更加可控；
    - 将显示的3D表示和一个神经渲染管道结果合并，这会实现更快的推理和更加逼真的图片。

    因此，将场景表示为合成的生成式神经特征场。

    ![image-20210910130925865](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910130925865.png)

    图1：概述。我们将场景表示为合成生成的神经特征场。

    具体过程：

    - 对于**随机采样的相机**，我们基于**单个特征场**对场景的特征图像进行**体积渲染**。
    - **2D神经渲染网络**将特征图像转换为RGB图像。

    具体效果：

    - 虽然只针对原始图像采集进行培训，但在测试时，我们能够**控制wrt的图像形成过程。摄影机姿势、对象姿势以及对象的形状和外观**。
    - 此外，我们的模型**超越了训练数据**，例如，我们可以合成具有比训练图像中更多对象的场景。请注意，为了清晰起见，我们将体积可视化为彩色，而不是特征（TODO：不懂）。

- 过程：我们将**场景体积渲染**volume render为相对低分辨率的特征图像，以节省时间和计算量。**神经渲染器neural renderer**处理这些特征图像并输出最终渲染。

- 效果：通过这种方式，我们的方法可以获得高质量的图像，**并可扩展到真实场景scales to real-world scenes**。我们发现，当对原始非结构化图像集合进行训练时，我们的方法允许对**单个对象和多对象场景进行可控的图像合成**。（多物体+真实场景+高分辨率+基于原始非结构化数据）



#### 2 Related Work

**基于GAN的图片合成：**

为了获取对合成过程更好的控制，许多工作研究了 **在没有明确监督的情况下**分离变异因素，而研究的方式无非是修改训练主题，要么是网络结构，又或者是研究investigate latent spaces of well-engineered and pre-trained generative models**精心设计和预先训练的生成模型的潜在空间**。

然而，这些工作，并没有**对场景的合成特点进行明确的建模**。

有一些工作也因此研究了合成过程如何在物体级别达到可控。 虽然实现了逼真的结果，但是上面提及的所有的work对图片信息的建模都是在2D层次，忽视了三维的机构。

因此，**这部分强调的是，对场景的合成特点的建模**

**Implicit Functions:**

使用隐式函数表示三维集合体在基于学习的3D重建中得到了广泛的应用，并且扩展到了场景级别的重建。

为了克服三维监控的需要，先前的几个工作提出不同的渲染技术，【61】提出了神经辐射场，它将一个隐含神经模型和体积渲染结合，用于复杂场景的新型视图合成。

基于该模型的表现力expressiveness，我们使用了NeRFs的生成式变体，作为本文物体级别的表示。**与我们的方法不同的是**，所讨论的工作需要**多视图图像**，以摄像机姿势作为监控，每个场景训练一个网络，并且不能生成新的场景。**相反**，我们从**非结构化图像**集合中学习生成模型，该模型允许**生成场景的可控照片级**真实感图像合成。

**3D-Aware Image Synthesis 三维感知图像合成:**

有一些工作研究了三维表示如何被当作归纳偏倚合并到生成式网络中。尽管许多方法使用额外的监控，但是我们专注于像我们的方法一样对原始图像收集进行训练的努力。

【32】学习了基于oxel的表示，通过不同的渲染技术；实现了三维可控，但由于立方体内存增长导致的体素分辨率有限，因此会出现伪影。

【63，64】提出了**体素化特征网格表示**，**通过重塑操作将其渲染为二维**。在取得令人印象深刻的结果的同时，**训练变得不太稳定**，结果也不太一致，**无法获得更高的分辨率**。

【46】将抽象特征与基本体和可微分渲染结合使用。在**处理多对象场景时，它们需要以纯背景图像的形式进行额外的监控，这对于真实场景来说是很难获得的**。

【77】提出了生成性神经辐射场（GRAF）。虽然在高分辨率下实现可控图像合成，但这种表示仅限于单对象场景，在更复杂的真实图像上效果会降低。

相比之下，我们将合成3D场景结构合并到生成模型中，以便它自然地处理多对象场景。此外，通过集成神经渲染管道，我们的模型可以扩展到更复杂的真实数据。

#### 3 Model

神经辐射场，是一个连续性的函数，它将**三维点坐标和一个视角方向**映射为一个体积密度值和一个RGB颜色值（三维），而【61，82】指出如果训练函数是一个神经网络，那么低纬度的两个输入必须被映射成高纬度特征才能表示复杂的信号。

其中有个特殊的设计——引入参数L，以元素相乘的方式，融入到各个角度中。

参数的意义在于：它通过引入一个归纳偏差，在规范方向上学习三维形状的表示，如果学习不大，那这个值将会是任意的。

![image-20210910163730156](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910163730156.png)



![image-20210910163621351](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910163621351.png)

而神经辐射场的模型，其实是一个多层感知机。

**生成式神经特征场**：

在神经辐射场的基础上，增加了两个方面：增加了潜在空间和将三维颜色输出转化为多维特征；

![image-20210910171715893](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210910171715893.png)

- 潜在空间，受到【77】的GRAF的启发，认为虽然MLP能够拟合单个场景中的多个姿势图像，但是认为表达能力还是比较受限，因此使用了潜在空间，具体的代码，就是两个正态分布的矩阵，经过处理后融入到原始数据中。
- 多维特征，目的其实不明。但是可以明确三个值不足以满足需求。

**Object Representation**

现有的一些模型的关键局限性在于，整个场景完全使用一个模型来表示。**而正如我们所感兴趣的是解开场景中的不同实体，因此我们需要对每个对象（包含背景）的pose，形状和外貌apperance进行控制。**因此，作者使用一个独立的特征场和一个仿射变换矩阵来表示每个物体，具体的是由缩放参数和平移参数以及旋转矩阵构成。

![image-20210911152628912](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210911152628912.png)

##### 3.2. Scene Compositions

##### 3.3. Scene Rendering

**3D Volume Rendering:**

volume render 就是一种渲染绘制三维数据的技术。[Volume Render](http://www.visitusers.org/index.php?title=Volume_Rendering#Ray_casting)通常用来绘制**几何图形**难以表现的流体、云、火焰、烟雾等效果。

**2D Neural Rendering:**

以往的工作希望得到一个RDB值，而当前工作得到一个多维特征向量，而多维特征向量协同输入图像，经过一个二维的卷积神经网络得到一个RGB值。**因此渲染操作，考虑了二维空间结构。**

GIRAFFE的整体结构

![image-20210911154530791](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210911154530791.png)

![image-20210911154640253](/home/cold/PaperReadFastly/PaperRead/论文阅读列表/CVPR2021/CVPR2021_abstarct.assets/image-20210911154640253.png)



